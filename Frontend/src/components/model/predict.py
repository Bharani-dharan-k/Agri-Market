import pandas as pd
import joblib
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

# Load dataset
df = pd.read_csv("market_prices.csv")

# Extract useful date-related features
df["Arrival_Date"] = pd.to_datetime(df["Arrival_Date"], errors="coerce")
df["Month"] = df["Arrival_Date"].dt.month
df["DayOfWeek"] = df["Arrival_Date"].dt.dayofweek  # 0 = Monday, 6 = Sunday

# Define seasons (India's cropping seasons)
def get_season(month):
    if month in [6, 7, 8, 9, 10]:  # Kharif season
        return 1
    elif month in [11, 12, 1, 2]:  # Rabi season
        return 2
    else:  # Zaid season (March-May)
        return 3

df["Season"] = df["Month"].apply(get_season)

# Price volatility (difference between max & min price)
df["PriceVolatility"] = df["Max Price"] - df["Min Price"]

# Rolling averages (7-day & 30-day)
df["RollingAvg_7"] = df["Modal Price"].rolling(7, min_periods=1).mean()
df["RollingAvg_30"] = df["Modal Price"].rolling(30, min_periods=1).mean()

# Drop NaN values generated by rolling averages
df.dropna(inplace=True)

# Encode categorical columns
mappings = {}
for col in ["State", "District", "Market", "Commodity", "Variety", "Grade"]:
    df[col] = df[col].astype("category").cat.codes
    mappings[col] = dict(enumerate(df[col].astype("category").cat.categories))

# Features and target variable
features = ["State", "District", "Market", "Commodity", "Variety", "Grade",
            "Month", "DayOfWeek", "Season", "PriceVolatility", "RollingAvg_7", "RollingAvg_30"]
X = df[features]
y = df["Modal Price"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a pipeline with feature scaling and model selection
rf = RandomForestRegressor(random_state=42)
xgb = XGBRegressor(objective="reg:squarederror", random_state=42)

pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("model", rf)  # Default model
])

# Hyperparameter tuning for RandomForest
param_grid = {
    "model": [rf, xgb],  # Try both models
    "model__n_estimators": [100, 200],
    "model__max_depth": [10, 20, None],
}

grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring="r2", n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model selection
best_model = grid_search.best_estimator_
print(f"✅ Best Model Selected: {best_model['model']}")

# Find best selling time based on historical price trends
best_selling_time = df.groupby("DayOfWeek")["Modal Price"].mean().idxmax()

# Save trained model and mappings
joblib.dump(best_model, "crop_price_model.pkl")
joblib.dump(mappings, "mapping_dicts.pkl")
joblib.dump(best_selling_time, "best_selling_time.pkl")

print("✅ Model trained with advanced techniques and saved successfully!")
